{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23ac6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import cv2, random\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import lightning as L\n",
    "import collections\n",
    "import cv2\n",
    "from zeus.monitor import ZeusMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c2da27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cpu.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "180e9378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 13:52:12,982] [zeus.monitor.energy](energy.py:223) Monitoring GPU indices [].\n",
      "[2025-05-27 13:52:12,982] [zeus.monitor.energy](energy.py:224) Monitoring CPU indices [0]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "monitor = ZeusMonitor(gpu_indices=None, cpu_indices=[0])            #gpu_indices=[torch.cuda.current_device()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30002eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNormalize(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNormalize, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate max value and standard deviation\n",
    "        max_val = torch.max(x)\n",
    "        std_dev = torch.std(x)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        eps = 1e-8\n",
    "        \n",
    "        # Normalize the vector\n",
    "        normalized = (x - torch.mean(x)) / (std_dev + eps)\n",
    "        normalized = normalized / (max_val + eps)\n",
    "        \n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d85ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_ = T.Compose([\n",
    "    T.Resize((224,224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_1 = T.Compose([CustomNormalize()])\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=4):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=num_heads,batch_first=True)\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, 1, input_dim))\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, input_dim))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add class token\n",
    "        batch_size = x.size(0)\n",
    "        class_token = self.class_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "\n",
    "        # Add position embedding\n",
    "        x = x + self.position_embedding\n",
    "\n",
    "        # Apply attention\n",
    "        x, _ = self.attention(x, x, x)\n",
    "        x = self.dropout(x)\n",
    "        return x[:, 0]  # Return class token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "953c5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DF_Detection_V2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DF_Detection_V2, self).__init__()\n",
    "        self.project_layer = nn.Linear(1000, 512) \n",
    "        self.generator_model = self.Gen_model\n",
    "        self.classifier_model = self.Classifier_model\n",
    "\n",
    "        \n",
    "        self.genlayer1 = nn.Linear(512, 256)\n",
    "        self.genlayer2 =  nn.Linear(256, 512)\n",
    "        self.genlayer3 = nn.Linear(512,512)\n",
    "\n",
    "        self.clflayer1 = nn.Linear(512,256)\n",
    "        self.attention_layer = AttentionLayer(input_dim=128)\n",
    "        self.clflayer3 = nn.Linear(128, 2) #3\n",
    "\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(512)\n",
    "        self.batchnorm_ = nn.BatchNorm1d(256)\n",
    "        self.layernorm = nn.LayerNorm(256)\n",
    "    \n",
    "    \n",
    "    def Gen_model(self, x):\n",
    "        x_1 = nn.GELU()(self.genlayer1(x)) #x + feat_\n",
    "        x_2 = nn.Dropout(0.25)(nn.GELU()(self.batchnorm(self.genlayer2(x_1))))\n",
    "        feat = self.genlayer3(x_2 + x)\n",
    "        return feat\n",
    "    \n",
    "    def Classifier_model(self, x_, t):\n",
    "        x_2 = (self.batchnorm_(self.clflayer1(x_ + nn.Dropout(0.5)(t))))\n",
    "        x_2 = self.layernorm(nn.Dropout(0.3)(nn.ELU()(x_2)))\n",
    "        x_3 = self.attention_layer(x_2.view(-1).reshape([len(x_),2,128]))\n",
    "        out = self.clflayer3(x_3 + x_2[:,:128] + x_2[:,128:])#out = self.clflayer3(x_3)\n",
    "        return  out\n",
    "      \n",
    "    def forward(self, x):\n",
    "        proj_x = (self.project_layer(x))\n",
    "        gen_out= self.generator_model(proj_x)\n",
    "        clf_out = self.classifier_model(proj_x, gen_out)\n",
    "        return proj_x, clf_out, gen_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cceb121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningDeepFakeDetection_V2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = DF_Detection_V2().eval()\n",
    "        # self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs)\n",
    "\n",
    "path ='DeepFake_Detection_V2-epoch=21-val_acc=0.88-val_loss=2.22_bb_finetuned_w_CMDFD.ckpt'\n",
    "model = LightningDeepFakeDetection_V2()#.load_from_checkpoint(path, map_location=\"cpu\").eval()\n",
    "# model.load_state_dict(torch.load(path, weights_only=True, map_location=device)).eval();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9576a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(path,map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "featmodel = LightningDeepFakeDetection_BB()#.load_from_checkpoint('models_lightning/BB/DeepFake_BB-epoch=11-val_acc=0.98-val_loss=0.46_FakeAVCeleb-v1.ckpt')\n",
    "featmodel.load_state_dict(torch.load('BB_mobilnet_weights_combined.pth', weights_only=False, map_location=torch.device('cpu')))\n",
    "featmodel.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8898aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_frames(video_path: str, n: int):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Cannot open video.\")\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total == 0:\n",
    "        raise RuntimeError(\"Empty / corrupt video.\")\n",
    "    n = min(n, total)\n",
    "    idxs = random.sample(range(total), n)\n",
    "    frames = []\n",
    "    for i in idxs:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ok, frame = cap.read()\n",
    "        if not ok: continue\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)     # BGR → RGB\n",
    "        frames.append(Image.fromarray(frame))\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def pred_cred(file):\n",
    "    test1 = sample_frames(file, 10) #'id07251-id07223.mp4'\n",
    "    test2 = sample_frames(file, 15)\n",
    "    test3 = sample_frames(file, 20)\n",
    "    batch1 = CustomNormalize()(torch.stack([featmodel(transform_(img).unsqueeze(0)) for img in test1]).squeeze(1))\n",
    "    batch2 = CustomNormalize()(torch.stack([featmodel(transform_(img).unsqueeze(0)) for img in test2]).squeeze(1))\n",
    "    batch3 = CustomNormalize()(torch.stack([featmodel(transform_(img).unsqueeze(0)) for img in test3]).squeeze(1))\n",
    "    with torch.no_grad():\n",
    "        _,logits,_ = model((batch1))                       # (B,10)\n",
    "        preds1 = logits.argmax(1).tolist()           # list[int]\n",
    "        _,logits,_ = model((batch2))                       # (B,10)\n",
    "        preds2 = logits.argmax(1).tolist()           # list[int]\n",
    "        _,logits,_ = model((batch3))                       # (B,10)\n",
    "        preds3 = logits.argmax(1).tolist()           # list[int]\n",
    "    # majority vote for overall prediction\n",
    "    majority1 = collections.Counter(preds1).most_common(1)[0][0]\n",
    "    majority2 = collections.Counter(preds2).most_common(1)[0][0]\n",
    "    majority3 = collections.Counter(preds3).most_common(1)[0][0]\n",
    "    majority = (majority1 + majority2 + majority3)/3\n",
    "    return int(majority)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e6c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cred('id07251-id07223.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb90b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = CustomNormalize()(torch.stack([featmodel(transform_(img).unsqueeze(0)) for img in test]).squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14212d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c68c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _,logits,_ = model((batch))                       # (B,10)\n",
    "    preds = logits.argmax(1).tolist()           # list[int]\n",
    "\n",
    "# majority vote for overall prediction\n",
    "majority = collections.Counter(preds).most_common(1)[0][0]\n",
    "\n",
    "# return {\"frame_predictions\": preds, \"majority_digit\": majority}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa1b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "974bd102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ced14/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from opt_inf_pip import pred_cred\n",
    "label = pred_cred(\n",
    "    video_path=\"id01126_QMyzehv4zsg.mp4\",\n",
    "    # model=classifier,\n",
    "    # featmodel=backbone,\n",
    "    # transform_=img_tfms,\n",
    "    # n_list=(10, 15, 20)\n",
    ")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1e9c084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ced14/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "# optimized_inference_pipeline.py\n",
    "\"\"\"Optimized CPU‑only inference pipeline for deep‑fake detection.\n",
    "\n",
    "Key optimizations\n",
    "-----------------\n",
    "1. **Single video pass** – sample all required frames in one sequential read to avoid costly random seeks.\n",
    "2. **Vectorised transforms** – apply image transforms and feature extraction in a batched fashion.\n",
    "3. **Single model call** – run the classifier once on the full set of frames, then slice logits for each subset.\n",
    "4. **Lightweight majority vote** – use `numpy` for fast computation.\n",
    "\n",
    "Assumptions\n",
    "-----------\n",
    "* `transform_`, `featmodel`, `model`, and `CustomNormalize` are already defined.\n",
    "* The classifier `model` expects a 2‑D tensor `(B, F)` and outputs `(None, logits, None)`.\n",
    "* All computation is on CPU; set the default tensor device accordingly.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import List, Sequence\n",
    "# import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torch, torch.nn as nn\n",
    "import cv2, random\n",
    "import torchvision.transforms as T\n",
    "# import lightning as L\n",
    "import collections\n",
    "\n",
    "__all__ = [\n",
    "    \"sample_frames\",\n",
    "    \"frame_indices\",\n",
    "    \"pred_cred\",\n",
    "]\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class LightningDeepFakeDetection_BB(nn.Module):#L.ightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', weights=None)\n",
    "        self.clf_layer = nn.Linear(1000,2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.model(inputs)\n",
    "        outputs = self.clf_layer(nn.Dropout(0.5)(nn.GELU()(inputs)))\n",
    "        return inputs\n",
    "    \n",
    "class CustomNormalize(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNormalize, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate max value and standard deviation\n",
    "        max_val = torch.max(x)\n",
    "        std_dev = torch.std(x)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        eps = 1e-8\n",
    "        \n",
    "        # Normalize the vector\n",
    "        normalized = (x - torch.mean(x)) / (std_dev + eps)\n",
    "        normalized = normalized / (max_val + eps)\n",
    "        \n",
    "        return normalized\n",
    "\n",
    "transform_ = T.Compose([\n",
    "    T.Resize((224,224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_1 = T.Compose([CustomNormalize()])\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=4):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=num_heads,batch_first=True)\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, 1, input_dim))\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, input_dim))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add class token\n",
    "        batch_size = x.size(0)\n",
    "        class_token = self.class_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "\n",
    "        # Add position embedding\n",
    "        x = x + self.position_embedding\n",
    "\n",
    "        # Apply attention\n",
    "        x, _ = self.attention(x, x, x)\n",
    "        x = self.dropout(x)\n",
    "        return x[:, 0]  # Return class token\n",
    "\n",
    "class DF_Detection_V2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DF_Detection_V2, self).__init__()\n",
    "        self.project_layer = nn.Linear(1000, 512) \n",
    "        self.generator_model = self.Gen_model\n",
    "        self.classifier_model = self.Classifier_model\n",
    "        \n",
    "        self.genlayer1 = nn.Linear(512, 256)\n",
    "        self.genlayer2 =  nn.Linear(256, 512)\n",
    "        self.genlayer3 = nn.Linear(512,512)\n",
    "\n",
    "        self.clflayer1 = nn.Linear(512,256)\n",
    "        self.attention_layer = AttentionLayer(input_dim=128)\n",
    "        self.clflayer3 = nn.Linear(128, 2) #3\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(512)\n",
    "        self.batchnorm_ = nn.BatchNorm1d(256)\n",
    "        self.layernorm = nn.LayerNorm(256)\n",
    "    \n",
    "    def Gen_model(self, x):\n",
    "        x_1 = nn.GELU()(self.genlayer1(x)) #x + feat_\n",
    "        x_2 = nn.Dropout(0.25)(nn.GELU()(self.batchnorm(self.genlayer2(x_1))))\n",
    "        feat = self.genlayer3(x_2 + x)\n",
    "        return feat\n",
    "    \n",
    "    def Classifier_model(self, x_, t):\n",
    "        x_2 = (self.batchnorm_(self.clflayer1(x_ + nn.Dropout(0.5)(t))))\n",
    "        x_2 = self.layernorm(nn.Dropout(0.3)(nn.ELU()(x_2)))\n",
    "        x_3 = self.attention_layer(x_2.view(-1).reshape([len(x_),2,128]))\n",
    "        out = self.clflayer3(x_3 + x_2[:,:128] + x_2[:,128:])#out = self.clflayer3(x_3)\n",
    "        return  out\n",
    "      \n",
    "    def forward(self, x):\n",
    "        proj_x = (self.project_layer(x))\n",
    "        gen_out= self.generator_model(proj_x)\n",
    "        clf_out = self.classifier_model(proj_x, gen_out)\n",
    "        return proj_x, clf_out, gen_out\n",
    "\n",
    "class LightningDeepFakeDetection_V2(nn.Module):#L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = DF_Detection_V2().eval()\n",
    "        # self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs)\n",
    "    \n",
    "path ='DeepFake_Detection_V2-epoch=21-val_acc=0.88-val_loss=2.22_bb_finetuned_w_CMDFD.ckpt' \n",
    "model = LightningDeepFakeDetection_V2()#.load_from_checkpoint(path, map_location=device).eval()\n",
    "checkpoint = torch.load(path,map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval();\n",
    "\n",
    "featmodel = LightningDeepFakeDetection_BB()#.load_from_checkpoint('models_lightning/BB/DeepFake_BB-epoch=11-val_acc=0.98-val_loss=0.46_FakeAVCeleb-v1.ckpt')\n",
    "featmodel.load_state_dict(torch.load('BB_mobilnet_weights_combined.pth', map_location=device))\n",
    "featmodel.eval();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a29fb55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_indices(total: int, k: int) -> List[int]:\n",
    "    \"\"\"Return *k* unique random indices in the range ``[0, total)``.\n",
    "\n",
    "    The output is **sorted** so that frames can be read sequentially.\n",
    "    \"\"\"\n",
    "    if total == 0:\n",
    "        raise RuntimeError(\"Empty / corrupt video.\")\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"k must be positive\")\n",
    "    k = min(k, total)\n",
    "    return sorted(random.sample(range(total), k))\n",
    "\n",
    "def sample_frames(video_path: str, idxs: Sequence[int]) -> List[Image.Image]:\n",
    "    \"\"\"Read the frames at *idxs* (must be sorted) and return them as ``PIL.Image``s.\n",
    "\n",
    "    Frames are fetched **sequentially** for maximal throughput on CPU‑only setups.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Cannot open video: %s\" % video_path)\n",
    "\n",
    "    frames: List[Image.Image] = []\n",
    "    next_idx_iter = iter(idxs)\n",
    "    next_target = next(next_idx_iter, None)\n",
    "    cur = 0\n",
    "\n",
    "    while next_target is not None:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break  # reached EOF prematurely\n",
    "        if cur == next_target:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(Image.fromarray(frame))\n",
    "            next_target = next(next_idx_iter, None)\n",
    "        cur += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) != len(idxs):\n",
    "        raise RuntimeError(\"Failed to retrieve all requested frames.\")\n",
    "    return frames\n",
    "\n",
    "def _batched_features(frames: Sequence[Image.Image]) -> torch.Tensor:\n",
    "    \"\"\"Apply preprocessing + feature network in one go.\"\"\"\n",
    "    batch = torch.stack([transform_(img) for img in frames])  # (B, C, H, W)\n",
    "    with torch.no_grad() and torch.inference_mode():\n",
    "            feats = featmodel(batch)  # (B, F)\n",
    "    return CustomNormalize()(feats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9dfa3570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 13:56:46,555] [zeus.monitor.energy](energy.py:223) Monitoring GPU indices [].\n",
      "[2025-05-27 13:56:46,555] [zeus.monitor.energy](energy.py:224) Monitoring CPU indices []\n"
     ]
    }
   ],
   "source": [
    "from zeus.monitor import ZeusMonitor\n",
    "import torch\n",
    "\n",
    "# GPU-only monitoring – let Zeus ignore the CPU\n",
    "monitor = ZeusMonitor(              # omit gpu_indices or set them as you like\n",
    "    cpu_indices=[],                 # **important change**\n",
    "    approx_instant_energy=True      # optional: better estimate for short windows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df68094d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire testing: 0.0 s, 0 J\n"
     ]
    }
   ],
   "source": [
    "video_path = 'id07251-id07223.mp4'\n",
    "subset_sizes = (15)#(15,15,15)\n",
    "total = int(cv2.VideoCapture(video_path).get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# max_k = max(subset_sizes)\n",
    "idxs = frame_indices(total, subset_sizes)\n",
    "idxs_2 = frame_indices(total, subset_sizes)\n",
    "idxs_3 = frame_indices(total, subset_sizes)\n",
    "# read frames & extract features\n",
    "frames_1 = sample_frames(video_path, idxs)\n",
    "frames_2 = sample_frames(video_path, idxs_2)\n",
    "frames_3 = sample_frames(video_path, idxs_3)\n",
    "feats = _batched_features(frames_1)  # (max_k, F)\n",
    "feats_2 = _batched_features(frames_2) \n",
    "feats_3 = _batched_features(frames_3) \n",
    "\n",
    "# classifier – single forward pass\n",
    "monitor.begin_window(\"testing\")\n",
    "with torch.no_grad() and torch.inference_mode():\n",
    "        _, logits1, _ = model(feats)\n",
    "        _, logits2, _ = model(feats_2)\n",
    "        _, logits3, _ = model(feats_3)\n",
    "        logits = (logits1 + logits2 + logits3)/3\n",
    "preds = logits.argmax(1).cpu().numpy()  # (max_k,)\n",
    "\n",
    "# preds2 = logits2.argmax(1).cpu().numpy()  # (max_k,)\n",
    "# preds3 = logits3.argmax(1).cpu().numpy()  # (max_k,)\n",
    "measurement = monitor.end_window(\"testing\")\n",
    "print(f\"Entire testing: {measurement.time} s, {measurement.total_energy} J\")\n",
    "\n",
    "# # majority vote per subset\n",
    "# majorities = [collections.Counter(preds[:k]).most_common(1)[0][0] for k in subset_sizes]\n",
    "# # final decision – majority of majorities\n",
    "# final_pred =  int(round(sum(majorities) / len(majorities))) #int(round(np.mean(majorities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04af7ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d04e6335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(preds).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f4619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f24e6878",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m preds = logits.argmax(\u001b[32m1\u001b[39m).cpu().numpy()  \u001b[38;5;66;03m# (max_k,)\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# majority vote per subset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m majorities = \u001b[43m[\u001b[49m\u001b[43mcollections\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCounter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmost_common\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubset_sizes\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# final decision – majority of majorities\u001b[39;00m\n\u001b[32m      9\u001b[39m final_pred =  \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28msum\u001b[39m(majorities) / \u001b[38;5;28mlen\u001b[39m(majorities))) \u001b[38;5;66;03m#int(round(np.mean(majorities)))\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# classifier – single forward pass\n",
    "with torch.no_grad() and torch.inference_mode():\n",
    "        _, logits, _ = model(feats)\n",
    "preds = logits.argmax(1).cpu().numpy()  # (max_k,)\n",
    "\n",
    "# majority vote per subset\n",
    "majorities = [collections.Counter(preds[:k]).most_common(1)[0][0] for k in subset_sizes]\n",
    "# final decision – majority of majorities\n",
    "final_pred =  int(round(sum(majorities) / len(majorities))) #int(round(np.mean(majorities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b2d7336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ff54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_cred(video_path: str, subset_sizes: Sequence[int] = (15, 20, 30)) -> int:\n",
    "    \"\"\"Return the majority‑voted prediction for *video_path*.\n",
    "\n",
    "    The video is scanned **once**.  The largest subset size dictates the number of\n",
    "    sampled frames; smaller subsets reuse the prefix of that sample for voting.\n",
    "    \"\"\"\n",
    "    # open once just to know frame count (cheap)\n",
    "    total = int(cv2.VideoCapture(video_path).get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    max_k = max(subset_sizes)\n",
    "    idxs = frame_indices(total, max_k)\n",
    "\n",
    "    # read frames & extract features\n",
    "    frames = sample_frames(video_path, idxs)\n",
    "    feats = _batched_features(frames)  # (max_k, F)\n",
    "\n",
    "    # classifier – single forward pass\n",
    "    with torch.no_grad() and torch.inference_mode():\n",
    "            _, logits, _ = model(feats)\n",
    "    preds = logits.argmax(1).cpu().numpy()  # (max_k,)\n",
    "\n",
    "    # majority vote per subset\n",
    "    majorities = [collections.Counter(preds[:k]).most_common(1)[0][0] for k in subset_sizes]\n",
    "    # final decision – majority of majorities\n",
    "    final_pred =  int(round(sum(majorities) / len(majorities))) #int(round(np.mean(majorities)))\n",
    "    return final_pred\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
