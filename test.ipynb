{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ac6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import cv2, random\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import lightning as L\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e9378",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "282cdfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningDeepFakeDetection_BB(L.LightningModule):\n",
    "    def __init__(self, lr=2e-5):\n",
    "        super().__init__()\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "        self.clf_layer = nn.Linear(1000,2)\n",
    "        # self.lr = lr\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.model(inputs)\n",
    "        outputs = self.clf_layer(nn.Dropout(0.5)(nn.GELU()(inputs)))\n",
    "        return outputs, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30002eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNormalize(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNormalize, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate max value and standard deviation\n",
    "        max_val = torch.max(x)\n",
    "        std_dev = torch.std(x)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        eps = 1e-8\n",
    "        \n",
    "        # Normalize the vector\n",
    "        normalized = (x - torch.mean(x)) / (std_dev + eps)\n",
    "        normalized = normalized / (max_val + eps)\n",
    "        \n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d85ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_ = T.Compose([\n",
    "    T.Resize((224,224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=4):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=num_heads,batch_first=True)\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, 1, input_dim))\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, input_dim))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add class token\n",
    "        batch_size = x.size(0)\n",
    "        class_token = self.class_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "\n",
    "        # Add position embedding\n",
    "        x = x + self.position_embedding\n",
    "\n",
    "        # Apply attention\n",
    "        x, _ = self.attention(x, x, x)\n",
    "        x = self.dropout(x)\n",
    "        return x[:, 0]  # Return class token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "953c5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DF_Detection_V2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DF_Detection_V2, self).__init__()\n",
    "        self.project_layer = nn.Linear(1000, 512) \n",
    "        self.generator_model = self.Gen_model\n",
    "        self.classifier_model = self.Classifier_model\n",
    "\n",
    "        \n",
    "        self.genlayer1 = nn.Linear(512, 256)\n",
    "        self.genlayer2 =  nn.Linear(256, 512)\n",
    "        self.genlayer3 = nn.Linear(512,512)\n",
    "\n",
    "        self.clflayer1 = nn.Linear(512,256)\n",
    "        self.attention_layer = AttentionLayer(input_dim=128)\n",
    "        self.clflayer3 = nn.Linear(128, 2) #3\n",
    "\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(512)\n",
    "        self.batchnorm_ = nn.BatchNorm1d(256)\n",
    "        self.layernorm = nn.LayerNorm(256)\n",
    "    \n",
    "    \n",
    "    def Gen_model(self, x):\n",
    "        x_1 = nn.GELU()(self.genlayer1(x)) #x + feat_\n",
    "        x_2 = nn.Dropout(0.25)(nn.GELU()(self.batchnorm(self.genlayer2(x_1))))\n",
    "        feat = self.genlayer3(x_2 + x)\n",
    "        return feat\n",
    "    \n",
    "    def Classifier_model(self, x_, t):\n",
    "        x_2 = (self.batchnorm_(self.clflayer1(x_ + nn.Dropout(0.5)(t))))\n",
    "        x_2 = self.layernorm(nn.Dropout(0.3)(nn.ELU()(x_2)))\n",
    "        x_3 = self.attention_layer(x_2.view(-1).reshape([len(x_),2,128]))\n",
    "        out = self.clflayer3(x_3 + x_2[:,:128] + x_2[:,128:])#out = self.clflayer3(x_3)\n",
    "        return  out\n",
    "      \n",
    "    def forward(self, x):\n",
    "        proj_x = (self.project_layer(x))\n",
    "        gen_out= self.generator_model(proj_x)\n",
    "        clf_out = self.classifier_model(proj_x, gen_out)\n",
    "        return proj_x, clf_out, gen_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningDeepFakeDetection_V2(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = DF_Detection_V2().eval()\n",
    "        # self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs)\n",
    "    \n",
    "path ='DeepFake_Detection_V2-epoch=21-val_acc=0.88-val_loss=2.22_bb_finetuned_w_CMDFD.ckpt' \n",
    "model = LightningDeepFakeDetection_V2.load_from_checkpoint(path, map_location=\"cpu\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04ab26f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ced14/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "featmodel = LightningDeepFakeDetection_BB()#.load_from_checkpoint('models_lightning/BB/DeepFake_BB-epoch=11-val_acc=0.98-val_loss=0.46_FakeAVCeleb-v1.ckpt')\n",
    "featmodel.load_state_dict(torch.load('BB_mobilnet_weights_combined.pth', weights_only=False, map_location=torch.device('cpu')))\n",
    "featmodel.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d2b7b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_frames(video_path: str, n: int):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Cannot open video.\")\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total == 0:\n",
    "        raise RuntimeError(\"Empty / corrupt video.\")\n",
    "    n = min(n, total)\n",
    "    idxs = random.sample(range(total), n)\n",
    "    frames = []\n",
    "    for i in idxs:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ok, frame = cap.read()\n",
    "        if not ok: continue\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)     # BGR â†’ RGB\n",
    "        frames.append(Image.fromarray(frame))\n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8898aac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
